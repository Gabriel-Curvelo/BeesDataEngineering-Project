### üîç Projeto de Pipeline de Dados com Arquitetura Medallion
Este projeto tem como objetivo construir uma pipeline de dados que consome informa√ß√µes de uma API p√∫blica, realiza transforma√ß√µes e persiste os dados em um data lake, seguindo o padr√£o arquitetura Medallion, com tr√™s camadas:

Bronze (bronzelayer): armazenamento dos dados brutos coletados diretamente da fonte.

Silver (silverlayer): dados curados, colunar, particionados por localiza√ß√£o.

Gold (goldlayer): camada anal√≠tica agregada, com dados prontos para consumo por ferramentas de BI ou an√°lises mais complexas.

### ‚öôÔ∏è Ferramentas Utilizadas
Todo o ecossistema foi constru√≠do com ferramentas open source e escal√°veis, permitindo f√°cil adapta√ß√£o a ambientes maiores e produtivos:

Docker

Apache Airflow

MinIO (Data Lake)

DuckDB

DBeaver (client SQL)

Python + PySpark

### üìà Monitoramento e Qualidade dos Dados
Para garantir a confiabilidade do pipeline, seria poss√≠vel implementar um sistema de monitoramento composto por:

Alertas autom√°ticas via Email ou outros canais, integrados ao Airflow, em caso de falhas de execu√ß√£o.

Valida√ß√µes de schema e integridade dos dados nas etapas de transforma√ß√£o.

Controle de logs salvos em arquivos e visualiza√ß√£o dos dashboards de execu√ß√£o no Airflow UI.

Dashboards de qualidade de dados dependendo da criticidade do pipeline.

### üîê Sobre as Credenciais
As credenciais de acesso est√£o expostas no reposit√≥rio com o √∫nico prop√≥sito de facilitar testes locais. Em produ√ß√£o, recomenda-se utilizar:

Gerenciadores de segredo (como HashiCorp Vault, AWS Secrets Manager, Databricks Secrets, GCP Secret Manager)

Vari√°veis de ambiente seguras

Criptografia de arquivos de configura√ß√£o

### ‚ñ∂Ô∏è Passo a passo para execu√ß√£o

1. **Pr√©-requisitos**:
   - Instale o [Docker](https://www.docker.com)
   - Instale o Python 3.9 ou superior
   - Instale o DBeaver ou outra ferramenta compat√≠vel com DuckDB: [https://dbeaver.io]

2. **Instale o Astro no PowerShell (Windows)** com o comando:
   ```
   winget install -e --id Astronomer.Astro
   ```
   **O que √© Astro?**
   > O Astro √© uma solu√ß√£o em nuvem que ajuda voc√™ a gastar menos tempo gerenciando o Apache Airflow¬Æ, com recursos que permitem criar, executar e observar dados em um s√≥ lugar.

3. **Clone o reposit√≥rio GitHub** no editor de sua prefer√™ncia:
   [https://github.com/Gabriel-Curvelo/BeesDataEngineering-Project]

4. **No terminal, v√° at√© a pasta `Airflow` e execute:**
   ```
   astro dev start
   ```
   Esse comando criar√° um container com:
   - Airflow: plataforma de gerenciamento de fluxos de trabalho.
   - DuckDB: sistema de banco de dados colunar embutido.

5. **Acesse o Airflow** via:
   http://localhost:8080
   Usu√°rio: `admin` | Senha: `admin`
   > ‚ö†Ô∏è Credenciais expostas propositalmente para ambiente de desenvolvimento local. Essas senhas s√£o fornecidas pela pr√≥pria documenta√ß√£o do Astro.

6. **Agora v√° at√© a pasta `DataLake` e execute:**
   ```bash
   docker-compose up -d
   ```
   Esse comando criar√° um container com o **MinIO**: plataforma de armazenamento de arquivos compat√≠vel com S3.

7. **Acesse o MinIO** via:
   http://localhost:9001
   Usu√°rio: `datalake` | Senha: `datalake`

8. **Crie os buckets no modelo Medallion** com os nomes:
   - `bronzelayer`
   - `silverlayer`
   - `goldlayer`

9. **No Airflow, execute a DAG `brewery`.**

10. **Ao t√©rmino da execu√ß√£o, os dados estar√£o carregados nas respectivas camadas.**

### üìä Extra: Visualiza√ß√£o dos Dados

Para facilitar a visualiza√ß√£o dos dados na camada Gold, foi utilizado o DuckDB. Ao final da execu√ß√£o da DAG, ser√° gerado um arquivo no caminho:

```
\BeesDataEngineering-Project\Airflow\include\minio.duckdb
```

Voc√™ pode:

- Copiar esse arquivo para uma pasta de sua prefer√™ncia
- Abrir o DBeaver e criar uma conex√£o direta com DuckDB
- Utilizar o caminho copiado como path do banco de dados DuckDB
- Visualizar os dados e gerar consultas SQL como desejar

**Alternativamente**, √© poss√≠vel configurar o acesso S3 diretamente no DuckDB (sem baixar o arquivo), assim como √© feito com o boto3, para acessar os dados diretamente da camada Gold no MinIO.

---

### üìÇ Organiza√ß√£o dos Scripts

- **Scripts dos ETLs**: `\BeesDataEngineering-Project\Airflow\include\scripts`
- **Scripts das DAGs**: `\BeesDataEngineering-Project\Airflow\dags`

                      




